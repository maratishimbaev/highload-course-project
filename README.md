# Курсовой проект по курсу HighLoad

## Содержание

[1. Выбор темы](#1) <br>
[2. Определение возможного диапазона нагрузок подобного проекта](#2) <br>
[3. Выбор планируемой нагрузки](#3) <br>
[4. Логическая схема базы данных](#4) <br>
[5. Физическая система хранения](#5) <br>
[6. Выбор прочих технологий](#6) <br>
[7. Расчет нагрузки и потребного оборудования](#7) <br>
[8. Выбор хостинга / облачного провайдера и расположения серверов](#8) <br>
[9. Схема балансировки нагрузки](#9) <br>
[10. Обеспечение отказоустойчивости](#10) <br>

## 1. <a name="1"></a> Выбор темы

Мессенджер (Telegram).

## 2. <a name="2"></a> Определение возможного диапазона нагрузок подобного проекта

Доля пользователей из России: [73%](https://exlibris.ru/news/telegram-2020-auditoriya-i-kanaly/). <br>

Ежемесячная аудитория: [30.4 млн](https://webindex.mediascope.net/report?byGeo=2&byDevice=3&byDevice=1&byDevice=2&byMonth=202011&id=240443). <br>
Ежедневная аудитория: [10.3 млн](https://webindex.mediascope.net/report?byGeo=2&byDevice=3&byDevice=1&byDevice=2&byMonth=202011&id=240443). <br>
Среднее время пользования сервисом: [7 минут в день](https://exlibris.ru/news/telegram-2020-auditoriya-i-kanaly/).

Рассчитаем количество уникальных пользователей в секунду. <br>
Примем количество пользователей в час за 10% ежедневной аудитории, тогда: `10.3 * 1e6 * 0.1 = 1.03 * 1e6 пол./час = 17166 пол./мин`. <br>
Учитывая среднее время пользования сервисом получим: `17166 * 7 = 120 * 1е3 пол./мин = 2000 пол./c`.

## 3. <a name="3"></a> Выбор планируемой нагрузки

Ежедневно пользователи Telegram отправляют [70 млрд](https://relayto.com/relayto/telegram-open-network-ton-ico-whitepaper-6kf4rycn/pdf) * 0.73 = 51.1 млрд сообщений. <br>

Запросов в час: `51,100,000,000 / 24 = 2,129,166,667`. <br>
Запросов в минуту: `2,129,166,667 / 60 = 35,486,111`. <br>
Запросов в секунду: `35,486,111 / 60 = 591,435`.

## 4. <a name="4"></a> Логическая схема базы данных

![](schema.png)

## 5. <a name="5"></a> Физическая система хранения

Для хранения сообщений и чатов будем использовать PostgreSQL.
Его преимущество над другими реляционными СУБД в более надежных и эффективных механизмах транзакции и репликации. <br>
Так как один сервер PostgreSQL не выдержит планируемую нагрузку, выполним шардинг таблицы сообщений.
Шардинг таблицы сообщений будем выполнять по полю chat_id. <br>
Для более быстрого доступа к данным будет необходимо использовать индексы.
Для таблицы message индекс будет создан по полям chat_id и created. Для таблиц chat и user - по полю id.

Текущие сессии пользователей будем хранить в Redis.
Он хранит все данные в памяти, что сильно увеличивает производительность в сравнении с реляционными СУБД.
В отличие от аналогов Redis позволяет хранить не только строки, но и более сложные типы данные: массивы, словари и множества.
Redis поддерживает master-slave репликацию, которая не приводит к блокировке ни главного, ни подчиненных серверов.

Также в Redis будем хранить список чатов каждого пользователя для более быстрого получения списка сообщений.
Ключом будет сессия пользователя, значением - список id чатов данного пользователя.
Шардинг будем выполнять при помощи разбиения хэша.

## 6. <a name="6"></a> Выбор прочих технологий

Для фронтенда будем использовать JavaScript/TypeScript.

Бэкенд будем писать на языке Golang, так как он имеет:
- простой синтаксис;
- удобную обработку многопоточности;
- интеграцию с выбранной базой данных.

Для общения между микросервисами на бэкенде будем использовать протокол gRPC.

## 7. <a name="7"></a> Расчет нагрузки и потребного оборудования

**Фронтенд**

Для объема фронтенда не больше 3 МБ и известной ежедневной аудитории получим
`3 * 10.3 * 1e6 = 30.8 * 1e6 МБ/день = 356 МБ/с`.

**База данных**

Рассчитаем средний объем одной записи в таблицах базы данных.

*message*

Среднюю длину сообщения возьмем за 80 символов.

&nbsp; | id | chat | author | text | created | updated | Итого
--- | --- | --- | --- | --- | --- | --- | ---
Объем (байт) | 8 | 8 | 8 | 80 | 8 | 8 | 120

*user*

&nbsp; | id | phone | username | name | Итого
--- | --- | --- | --- | --- | ---
Объем (байт) | 8 | 16 | 32 | 32 | 88

*chat*

За среднее количество пользователей в чате примем 3 человека.

&nbsp; | ids | name | Итого
--- | --- | --- | ---
Объем (байт) | 32 | 32 | 64

Зная количество ежедневно отправляемых сообщений вычислим объем для их записи в базу за один месяц:
`30 * 120 * 51.1 * 1e9 = 183.96 * 1e12 байт = 171,326 ГБ`.

На данный момент количество пользователей сервиса равно [400 млн](https://3dnews.ru/1009337) человек.
Ежедневно регистрируется около 1.5 млн новых пользователей. <br>
Тогда объем для записей пользователей: `88 * 400 * 1e6 = 32 ГБ`. <br>
Нагрузка на регистрацию новых пользователей: `1.5 * 1e6 rpd = 17 rps`. <br>
Нагрузка на авторизацию: `30.4 * 1e6 * 0.15 / 7 rpd = 30 rps` при 15% пользователей, что заходят реже, чем раз в неделю (для них истекает сессия).

Рассчитаем нагрузку на получение новых сообщений из базы (при 50 запусках приложения в среднем): `10.3 * 1e6 * 50 rpd = 5960 rps`.
Общая нагрузка: 597,395 rps.

Взяв за максимальную нагрузку на один сервер базы 5000 RPS, посчитаем количество машин: `597,395 / 5,000 = 120`.

Объем таблицы чатов: `64 * 400 * 1e6 = 23 ГБ`. <br>
Нагрузка на создание новых чатов: `30.4 * 1e6 rpd = 351 rps`. <br>
Нагрузка на чтение чатов: `10.3 * 1e6 * 50 rpd = 5960 rps`.

**Список чатов пользователя**

Посчитаем объем базы для записи списков: `30.4 * 1e6 * (8 + 100 * 8) = 22.8 ГБ` (для 100 чатов на пользователя).
Для запаса возьмем сервер с 64 GB памяти.

Пусть пользователь запрашивает сообщения 30 раз за день, тогда rps будет равно: `10.3 * 1e6 * 30 rpd = 3570 rps` <br>
Rps на создание нового чата будет заметно меньше, если даже пользователи будут добавлять по одному чату ежедневно, rps составит: `30.4 * 1e6 rpd = 351 rps`

Один сервер Redis выдержит такую нагрузку. Для надежности добавим к нему 2 реплики.

**Серверы приложений**

Согласно [бенчмарку](https://github.com/smallnest/go-web-framework-benchmark) сервер на Go выдает 30,000 rps. <br>
Из-за наличия бизнес-логики в серверах приложений, их производительность будет меньше, примем его за 10,000 rps. <br>
Для надежности возьмем по 2 дополнительных сервера к каждой машине.

**Балансировщик**

Рассчитаем количество балансировщиков (взяв 400 rps на ядро): `591,000 / 400 / 32 = 47 машин` по [32 ядра](https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/). <br> 
Учитывая терминацию SSL для 2000 пол./c добавим еще 8 ядер на каждый. <br>
На случай сбоев возьмем по одному дополнительному серверу для каждого балансировщика.

**Итоговое оборудование**

&nbsp; | CPU (cores) | RAM (GB) | SSD (GB) | Количество
--- | ---: | ---: | ---: | ---:
Фронтенд | 16 | 32 | 1024 | 2
Сообщения (master + slaves) | 16 | 64 | 1024 | 360/месяц
Авторизация (master + slaves) | 16 | 32 | 2048 | 6
Чаты (master + slaves) | 16 | 32 | 2048 | 6
Redis | 16 | 32 | 1024 | 3
Балансировщик | 40 | 64 | 1024 | 94
Сообщения (golang) | 32 | 16 | 1024 | 45
Авторизация (golang) | 32 | 16 | 1024 | 3
Чаты (golang) | 32 | 16 | 1024 | 3
Список чатов | 8 | 64 | 1024 | 3

## 8. <a name="8"></a> Выбор хостинга / облачного провайдера и расположения серверов

Так как основная часть российской аудитории расположена в европейской части, расположим все сервера в Москве.

## 9. <a name="9"></a> Схема балансировки нагрузки

Нагрузку балансируем при помощи nginx с использованием схемы L7.
С помощью алгоритма Round Robin будем выбирать сервер, на который будет отправлен запрос.

Балансировка между серверами nginx будет осуществляться при помощи L4 via API Tunneling.

## 10. <a name="10"></a> Обеспечение отказоустойчивости

Для обеспечения отказоустойчивости будем использовать репликацию master-slave - по 2 реплики на каждый сервер.
Данные на репликах будут появляться с некоторой задержкой от мастера.
Поэтому мастер будет отвечать за изменение, а реплики - за их чтение.
При выходе из строя мастера, одна из реплик станет им и будет принимать операции на изменение.
Если же выйдут из строя реплики, то все запросы будут направляться только в мастер.